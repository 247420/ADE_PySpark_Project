{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62ae1a-b219-47e8-b4ad-453d296622b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Original- und Zielpfade\n",
    "csv_path = \"../data/main_dataset.csv\"  # Eingabedatei\n",
    "output_file = \"../data/main_final_dataset.csv\"  # Endgültige Ausgabedatei\n",
    "\n",
    "# Öffne die Eingabedatei und führe beide Bereinigungen nacheinander durch\n",
    "with open(csv_path, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    for line in infile:\n",
    "        # Schritt 1: Entferne `;;;` am Ende der Zeile (falls vorhanden)\n",
    "        cleaned_line = line.rstrip()  # Entferne Leerzeichen oder Steuerzeichen\n",
    "        if cleaned_line.endswith(';;;'):\n",
    "            cleaned_line = cleaned_line[:-3]  # Entferne die letzten drei Zeichen\n",
    "        \n",
    "        # Schritt 2: Entferne äußere Anführungszeichen (nur am Anfang und Ende der Zeile)\n",
    "        if cleaned_line.startswith('\"') and cleaned_line.endswith('\"'):\n",
    "            cleaned_line = cleaned_line[1:-1]  # Entferne das erste und letzte Zeichen\n",
    "\n",
    "        # Schritt 3: Ersetze doppelte Anführungszeichen (\"\") durch einfache Anführungszeichen (\")\n",
    "        cleaned_line = cleaned_line.replace('\"\"', '\"')\n",
    "\n",
    "        # Schreibe die bereinigte Zeile in die Zieldatei\n",
    "        outfile.write(cleaned_line + \"\\n\")\n",
    "\n",
    "print(f\"Die Datei wurde komplett verarbeitet und unter {output_file} gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee970e4-c34f-4851-8d0a-238501156bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType, MapType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, size, when, round \n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PickleToDataFramewithRDD\").master(\"local[8]\").getOrCreate()\n",
    "\n",
    "# Verzeichnis mit den Pickle-Dateien\n",
    "pickle_dir = \"../data/100 Spotify Dataset\"\n",
    "# CSV-Datei in Spark laden\n",
    "csv_path = \"../data/main_final_dataset.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"meta\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"track\", StructType([\n",
    "        StructField(\"num_samples\", IntegerType(), True),\n",
    "        StructField(\"duration\", FloatType(), True),\n",
    "        StructField(\"loudness\", FloatType(), True),\n",
    "        StructField(\"tempo\", FloatType(), True),\n",
    "        StructField(\"tempo_confidence\", FloatType(), True),\n",
    "        StructField(\"key\", IntegerType(), True),\n",
    "        StructField(\"key_confidence\", FloatType(), True),\n",
    "        StructField(\"mode\", IntegerType(), True),\n",
    "        StructField(\"mode_confidence\", FloatType(), True),\n",
    "        StructField(\"time_signature\", IntegerType(), True),\n",
    "        StructField(\"time_signature_confidence\", FloatType(), True),\n",
    "    ]), True),\n",
    "    StructField(\"bars\", ArrayType(StructType([\n",
    "        StructField(\"start\", FloatType(), True),\n",
    "        StructField(\"duration\", FloatType(), True),\n",
    "        StructField(\"confidence\", FloatType(), True),\n",
    "    ])), True),\n",
    "    StructField(\"beats\", ArrayType(StructType([\n",
    "        StructField(\"start\", FloatType(), True),\n",
    "        StructField(\"duration\", FloatType(), True),\n",
    "        StructField(\"confidence\", FloatType(), True),\n",
    "    ])), True),\n",
    "    StructField(\"sections\", ArrayType(StructType([\n",
    "        StructField(\"start\", FloatType(), True),\n",
    "        StructField(\"duration\", FloatType(), True),\n",
    "        StructField(\"confidence\", FloatType(), True),\n",
    "        StructField(\"tempo\", FloatType(), True),\n",
    "        StructField(\"tempo_confidence\", FloatType(), True),\n",
    "        StructField(\"key\", IntegerType(), True),\n",
    "        StructField(\"key_confidence\", FloatType(), True),\n",
    "        StructField(\"mode\", IntegerType(), True),\n",
    "        StructField(\"mode_confidence\", FloatType(), True),\n",
    "        StructField(\"time_signature\", IntegerType(), True),\n",
    "        StructField(\"time_signature_confidence\", FloatType(), True),\n",
    "    ])), True),\n",
    "    StructField(\"segments\", ArrayType(StructType([\n",
    "        StructField(\"start\", FloatType(), True),\n",
    "        StructField(\"duration\", FloatType(), True),\n",
    "        StructField(\"confidence\", FloatType(), True),\n",
    "        StructField(\"loudness_start\", FloatType(), True),\n",
    "        StructField(\"loudness_max\", FloatType(), True),\n",
    "        StructField(\"loudness_max_time\", FloatType(), True),\n",
    "        StructField(\"pitches\", ArrayType(FloatType()), True),\n",
    "        StructField(\"timbre\", ArrayType(FloatType()), True),\n",
    "    ])), True),\n",
    "    StructField(\"tatums\", ArrayType(StructType([\n",
    "        StructField(\"start\", FloatType(), True),\n",
    "        StructField(\"duration\", FloatType(), True),\n",
    "        StructField(\"confidence\", FloatType(), True),\n",
    "    ])), True),\n",
    "    StructField(\"track_uri\", StringType(), True),\n",
    "])\n",
    "\n",
    "# CSV File einlesen: quote berücksichtigt das Inhalte zwischen \" \" als eine Spalte genommen werden\n",
    "csv_df = spark.read.option(\"header\", True).option(\"quote\", '\"').option(\"inferSchema\", True).csv(csv_path)\n",
    "# Alle Pickle-Dateien sammeln\n",
    "pickle_files = [os.path.join(pickle_dir, f) for f in os.listdir(pickle_dir) if f.endswith(\".pkl\") or f.endswith(\".pickle\")]\n",
    "\n",
    "# RDD erstellen basierend auf der Liste der Pickle-Dateien\n",
    "pickle_rdd = spark.sparkContext.parallelize(pickle_files)\n",
    "\n",
    "def process_pickle(filepath):\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as file:\n",
    "            pickle_data = pickle.load(file)\n",
    "\n",
    "        return {\n",
    "            \"track_uri\": pickle_data.get(\"track_uri\", \"Unbekannt\"),\n",
    "            \"bars\": pickle_data.get(\"bars\", []),\n",
    "            \"duration_ms\": pickle_data.get(\"duration_ms\", \"Unbekannt\"),\n",
    "            \"sections\": pickle_data.get(\"sections\", []),\n",
    "            \"segments\": pickle_data.get(\"segments\", []),\n",
    "            \"loudness_max\": pickle_data.get(\"loudness_max\", \"Unbekannt\"),\n",
    "            \"keys\": pickle_data.get(\"keys\", []),\n",
    "            \"track\": pickle_data.get(\"track\", [])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"track_uri\": None, \"error\": str(e)}\n",
    "\n",
    "# Pickle-Dateien parallel verarbeiten\n",
    "processed_rdd = pickle_rdd.map(process_pickle)\n",
    "\n",
    "# RDD in Spark DataFrame umwandeln\n",
    "pickle_df = spark.createDataFrame(processed_rdd, schema=schema)\n",
    "\n",
    "# Fehlerhafte Einträge herausfiltern\n",
    "valid_pickle_df = pickle_df.filter(pickle_df.track_uri.isNotNull())\n",
    "\n",
    "# CSV-Daten mit Pickle-Daten verbinden\n",
    "combined_df = valid_pickle_df.join(csv_df, valid_pickle_df[\"track_uri\"] == csv_df[\"track_uri_csv\"], how=\"inner\")\n",
    "\n",
    "# Join-Information fürs Debugging\n",
    "print(f\"JOIN Information:\")\n",
    "print(f\"- Zeilen vor Join: {valid_pickle_df.count()}\")\n",
    "print(f\"- Zeilen nach Join: {combined_df.count()}\")\n",
    "print(\"\")\n",
    "\n",
    "# Statistik anzeigen\n",
    "total_files = len(pickle_files)\n",
    "successful_files = valid_pickle_df.count()\n",
    "failed_files = total_files - successful_files\n",
    "\n",
    "print(f\"Statistik:\")\n",
    "print(f\"- Gesamtanzahl der Dateien: {total_files}\")\n",
    "print(f\"- Erfolgreich verarbeitet: {successful_files}\")\n",
    "print(f\"- Fehlerhaft: {failed_files}\")\n",
    "\n",
    "# 1. Berechnung des Durchschnittsmodus und Major-Prozentsatz für alle Songs\n",
    "# Erstelle eine neue Spalte, die 1 ist, wenn der Modus 'Major' (1) ist, sonst 0\n",
    "combined_df = combined_df.withColumn(\"is_major\", F.when(combined_df[\"mode\"] == 1, 1).otherwise(0))\n",
    "\n",
    "# Gruppiere nach einer geeigneten Ebene (falls erforderlich, z.B. über alle Daten)\n",
    "mode_stats = combined_df.agg(round(F.mean(\"is_major\")*100,2).alias(\"major_percentage\"))\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "mode_stats.show()\n",
    "\n",
    "# Berechnung der benötigten Spalten und Sortierung\n",
    "results_df = combined_df.select(\n",
    "    \"name\",\n",
    "    (col(\"duration_ms\").cast(\"float\") / 1000).alias(\"duration_sec\"),  # Dauer in Sekunden\n",
    "    when(col(\"duration_ms\") > 0, 1 / (size(col(\"sections\")) / (col(\"duration_ms\").cast(\"float\") / 1000))).otherwise(None).alias(\"Anzahl Sekunden pro Section (Kehrwert)\"),  #Kehrwert sections pro Sekunde \n",
    "    when(col(\"duration_ms\") > 0, size(col(\"segments\")) / (col(\"duration_ms\").cast(\"float\") / 1000)).otherwise(0).alias(\"segments_pro_sekunde\")\n",
    ").orderBy(col(\"Anzahl Sekunden pro Section (Kehrwert)\").desc())  # Sortierung nach Sekunden pro Section absteigend\n",
    "\n",
    "# Berechnung der Korrelation zwischen sections_pro_sekunde und segments_pro_sekunde\n",
    "correlation = results_df.stat.corr(\"Anzahl Sekunden pro Section (Kehrwert)\", \"segments_pro_sekunde\")\n",
    "\n",
    "# Ausgabe der Korrelation\n",
    "print(f\"Korrelation zwischen sections_pro_sekunde und segments_pro_sekunde: {correlation}\")\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "results_df.show(truncate=False)\n",
    "\n",
    "# Schritt 1: Sicherstellen, dass die Spalten den Typ \"double\" haben\n",
    "combined_df = combined_df.withColumn(\"energy\", combined_df[\"energy\"].cast(\"double\")).withColumn(\"danceability\", combined_df[\"danceability\"].cast(\"double\"))\n",
    "\n",
    "# Schritt 2: Explodieren der \"segments\"-Spalte und Extrahieren von \"loudness_max\"\n",
    "# Hier Zugriff auf Information der Pickle Datei: Da diese eine geschachtelte Struktur enthält, muss zunächst auf Segments zugegriffen werden und anschließend auf loudness max\n",
    "exploded_df = combined_df.withColumn(\"segment\", F.explode(\"segments\")).withColumn(\"loudness_max_segment\", F.col(\"segment.loudness_max\").cast(\"double\"))\n",
    "\n",
    "# Schritt 3: Filtern der Zeilen, um null-Werte zu vermeiden und Berechnung der Korrelationen\n",
    "filtered_df = exploded_df.filter((F.col(\"energy\").isNotNull()) & (F.col(\"danceability\").isNotNull()) & (F.col(\"loudness_max_segment\").isNotNull()))\n",
    "\n",
    "# Berechnung der Korrelationen\n",
    "correlation_energy_loudness_max = filtered_df.stat.corr(\"energy\", \"loudness_max_segment\")\n",
    "correlation_loudness_max_danceability = filtered_df.stat.corr(\"loudness_max_segment\", \"danceability\")\n",
    "correlation_energy_danceability = filtered_df.stat.corr(\"energy\", \"danceability\")\n",
    "\n",
    "# Ausgabe der Korrelationen\n",
    "print(f\"Korrelation zwischen Energy und Loudness Max: {correlation_energy_loudness_max}\")\n",
    "print(f\"Korrelation zwischen Loudness Max und Danceability: {correlation_loudness_max_danceability}\")\n",
    "print(f\"Korrelation zwischen Energy und Danceability: {correlation_energy_danceability}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Spark DataFrame vorbereiten\n",
    "filtered_df = filtered_df.select(\n",
    "    col(\"energy\").cast(\"float\"),\n",
    "    col(\"loudness_max_segment\").cast(\"float\"),\n",
    "    col(\"danceability\").cast(\"float\")\n",
    ").na.drop()\n",
    "\n",
    "# Sampling der Daten (10.000 Zeilen für Visualisierung)\n",
    "sample_size = 10000\n",
    "sampled_df = filtered_df.sample(fraction=sample_size / filtered_df.count())\n",
    "\n",
    "# Umwandeln der Stichprobe in Pandas DataFrame\n",
    "filtered_pd_df = sampled_df.toPandas()\n",
    "\n",
    "# Visualisierung 1: Hexbin-Dichteplot für Energy vs. Loudness Max\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hexbin(filtered_pd_df[\"energy\"], filtered_pd_df[\"loudness_max_segment\"], gridsize=50, cmap=\"viridis\", mincnt=1)\n",
    "plt.colorbar(label=\"Punktdichte\")\n",
    "plt.title(\"Dichte der Punkte: Energy vs. Loudness Max\")\n",
    "plt.xlabel(\"Energy\")\n",
    "plt.ylabel(\"Loudness Max\")\n",
    "plt.show()\n",
    "\n",
    "# Visualisierung 2: Hexbin-Dichteplot für Danceability vs. Loudness Max\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hexbin(filtered_pd_df[\"danceability\"], filtered_pd_df[\"loudness_max_segment\"], gridsize=50, cmap=\"viridis\", mincnt=1)\n",
    "plt.colorbar(label=\"Punktdichte\")\n",
    "plt.title(\"Dichte der Punkte: Danceability vs. Loudness Max\")\n",
    "plt.xlabel(\"Danceability\")\n",
    "plt.ylabel(\"Loudness Max\")\n",
    "plt.show()\n",
    "\n",
    "#Visualisierung 3: Energy vs. Danceability\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hexbin(filtered_pd_df[\"energy\"], filtered_pd_df[\"danceability\"], gridsize=50, cmap=\"viridis\", mincnt=1)\n",
    "plt.colorbar(label=\"Punktdichte\")\n",
    "plt.title(\"Dichte der Punkte: Energy vs. Danceability\")\n",
    "plt.xlabel(\"Energy\")\n",
    "plt.ylabel(\"Danceability\")\n",
    "plt.show()\n",
    "\n",
    "print(spark.conf.get(\"spark.master\"))  # Gibt den Master-Node aus (Standalone: spark/localhost:7077) (lokal: Je nach Einstellung) \n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Codeausführung abgeschlossen. Dauer: {execution_time_minutes:.2f} Minuten\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
